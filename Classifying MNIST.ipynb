{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abbaspour - 610398147 - Clustering MNIST\n",
    "The problem is reduce feature of MNIST dataset images with an autoencoder becomes tuned and trained, and use encoded images (from three set train set, test set and validation set (35% of test set)) to get classified through a neural network classifier.\n",
    "Firrst some methods of some libraries are imported for calculations and plottings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib.pyplot import figure, ion, legend, plot, savefig, scatter, show, subplot, subplots, title, xlabel, ylabel\n",
    "from numpy import argmax, argsort, random, reshape, sqrt\n",
    "from os import environ\n",
    "from pandas import read_csv\n",
    "from seaborn import histplot\n",
    "# from scipy.linalg import eigh\n",
    "# from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "environ[\"KMP_DUPLICATE_LIB_OK\"], environ[\"TF_CPP_MIN_LOG_LEVEL\"], environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"TRUE\", \"3\", \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset & Defining Variables (+ Answering Questions)\n",
    "Train set and test set are read as data frames. Accoring to histogram plot located in 'Evaluations' folder, whole dataset is roughly balanced and dataset doesn't seem to be resampled.\n",
    "As a preprocessing, values are divided by 255 as they're in range (0, 255) as a normalization.\n",
    "The reason of this action is to avoid the possibility of exploding gradients due to the high range the pixels and improve the convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading & normalizing the dataset\n",
    "trainSet, testSet = read_csv(\"mnist_train.csv\"), read_csv(\"mnist_test.csv\")\n",
    "df = trainSet._append(testSet, ignore_index = True)\n",
    "X_train, X_test, y_train, y_test, Optimizers, Activations, Initializers, Architecture, learningRates, lossFunction, Accuracy, Loss, validationAccuracy, validationLoss, Title, Alpha, localPopulation, Numbers, numberIterations, numberGenerations, Fitness, Parameters, Tuner = trainSet.drop([\"label\"], axis = 1).astype(\"float32\") / 255., testSet.drop([\"label\"], axis = 1).astype(\"float32\") / 255., trainSet[\"label\"].astype(\"int\"), testSet[\"label\"].astype(\"int\"), {Adam: \"Adam\" , Nadam: \"Nadam\", SGD: \"SGD\"}, [\"linear\", \"relu\", \"tanh\"], [\"glorot_normal\", \"glorot_uniform\", \"zero\"], (500, 100, 30), [0.001, 0.01, 0.1, 0.2, 0.25, 0.3, 0.5, 0.8], \"binary_crossentropy\", \"accuracy\", \"loss\", \"val_accuracy\", \"val_loss\", \"Autoencoder Evaluation\", 0.6, 1, 10, 1, 1, [], [], {}\n",
    "dfX_train, optimizerNames, localBound, number_test_samples, encoder_input_dimension, decoder_input_dimension, number_hidden_layers, Beta = X_train, list(Optimizers.values()), X_train.shape[1], len(X_test), Architecture[0], Architecture[-1], len(Architecture) - 1, 1 - Alpha\n",
    "\n",
    "# Defining the encoder input & decoder input dimentions.\n",
    "Image, Length, Start = Input(shape = (localBound,)), int(sqrt(localBound)), int(number_test_samples * 0.35)\n",
    "\n",
    "# 0.35% of test samples are chosen as validation samples\n",
    "validationSet = testSet.iloc[Start : number_test_samples - Start]\n",
    "y_val, X_val, arrtestSet = validationSet[\"label\"], validationSet.drop([\"label\"], axis = 1), testSet.to_numpy()\n",
    "print(f\"There're {len(X_train)} samples of {Length} X {Length} images as train samples and {number_test_samples} samples as test samples.\")\n",
    "\n",
    "# Plot samples\n",
    "for Number in range(Numbers):\n",
    "    fig = figure\n",
    "    imshow(arrX_train[Number], cmap = \"gray\")\n",
    "    savefig(dpi = 1200)\n",
    "    show()\n",
    "\n",
    "# Plotting raw data records\n",
    "Figure, ax = subplots()\n",
    "ax.pie(df[\"label\"].value_counts(), labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"], autopct = '%1.1f%%', shadow = True, startangle = 90)\n",
    "ax.axis(\"equal\")\n",
    "savefig(dpi = 1200)\n",
    "show()\n",
    "histplot(df[\"label\"])\n",
    "savefig(\"MNIST Histogram\", dpi = 1200)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Autoencoder\n",
    "Target is to minimizing binary crossentropy value as error function.\n",
    "Autoencoder is fine tuned by searching step by step between three optimizers 'Adam', 'Nadam' & 'SGD', weight initializers methods 'Glorot Normal', 'Glorot Uniform' and 'Zero' and activation functions 'Linear', 'Relu' and 'Tanh'.\n",
    "Architecture is (500, 100, 30) for hidden layer. Size of input and output of target autoencoder equals 784 because the program has to use and process all 28 x 28 = 784 pixels of each image of the dataset.\n",
    "When best optimizer is found by searching, model would be search for good learning rate between candidate learning rates.\n",
    "In tuning, batch size equals to 500 and search for each tuple of hyperparameters comes done in three epochs as a good group of hyperparameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning hyperparameters\n",
    "print(\"\\nTuning autoencoder...\")\n",
    "for Optimizer in optimizerNames:\n",
    "    for Initializer in Initializers:\n",
    "        for Activation in Activations:\n",
    "            # Define & build the encoder & decoder\n",
    "            decoderInput  = Input(shape = (decoder_input_dimension,))\n",
    "            Encoded, Decoded = Dense(encoder_input_dimension, activation = Activation, kernel_initializer = Initializer)(Image), Dense(Architecture[-2], activation = Activation, kernel_initializer = Initializer)(decoderInput)\n",
    "            for hiddenLayer in range(1, number_hidden_layers):\n",
    "                Encoded, Decoded = Dense(Architecture[hiddenLayer], activation = Activation, kernel_initializer = Initializer)(Encoded), Dense(Architecture[-(hiddenLayer + 2)], activation = Activation, kernel_initializer = Initializer)(Decoded)\n",
    "\n",
    "            Encoder, Decoder = Model(Image, Dense(decoder_input_dimension, activation = Activation)(Dense(Architecture[-2], activation = Activation)(Encoded)), name = \"Encoder\"), Model(decoderInput, Dense(localBound, activation = \"sigmoid\", kernel_initializer = Initializer)(Decoded), name = \"Decoder\")\n",
    "                \n",
    "            # Build main autoencoder\n",
    "            Autoencoder = Model(Image, Decoder(Encoder(Image)))\n",
    "\n",
    "            # Compile the autoencoder\n",
    "            Autoencoder.compile(loss = lossFunction, metrics = [\"accuracy\"], optimizer = Optimizer)\n",
    "\n",
    "            # Fit the data on the autoencoder\n",
    "            Autoencoder = Autoencoder.fit(X_train, X_train, batch_size = 500, epochs = 3, shuffle = True, validation_data = (X_val, X_val), verbose = 0)\n",
    "            Results = Autoencoder.history\n",
    "            Tuner[(Optimizer, Initializer, Activation)] = Alpha * (Results[validationAccuracy][0] - Results[validationLoss][0]) + Beta * (Results[Accuracy][0] - Results[Loss][0])\n",
    "\n",
    "Optimizer, Initializer, Activation = max(Tuner, key = Tuner.get)\n",
    "optimizerName = Optimizer\n",
    "Optimizer, decoderInput, Tuner = list(Optimizers.keys())[optimizerNames.index(optimizerName)], Input(shape = (decoder_input_dimension,)), {}\n",
    "\n",
    "# Define & build well-tuned encoder & decoder\n",
    "Encoded, Decoded = Dense(encoder_input_dimension, activation = Activation, kernel_initializer = Initializer)(Image), Dense(Architecture[-2], activation = Activation, kernel_initializer = Initializer)(decoderInput)\n",
    "for hiddenLayer in range(1, number_hidden_layers):\n",
    "    Encoded, Decoded = Dense(Architecture[hiddenLayer], activation = Activation, kernel_initializer = Initializer)(Encoded), Dense(Architecture[-(hiddenLayer + 2)], activation = Activation, kernel_initializer = Initializer)(Decoded)\n",
    "\n",
    "Encoder, Decoder = Model(Image, Dense(Architecture[-1], activation = Activation)(Dense(Architecture[-2], activation = Activation)(Encoded)), name = \"Encoder\"), Model(decoderInput, Dense(localBound, activation = \"sigmoid\", kernel_initializer = Initializer)(Decoded), name = \"Decoder\")\n",
    "\n",
    "# Search for good-enough learning rate\n",
    "for learningRate in learningRates:\n",
    "    # Build main autoencoder\n",
    "    Autoencoder = Model(Image, Decoder(Encoder(Image)))\n",
    "\n",
    "    # Compile the autoencoder\n",
    "    Autoencoder.compile(loss = lossFunction, metrics = [\"accuracy\"], optimizer = Optimizer(learning_rate = learningRate))\n",
    "\n",
    "    # Fit the data on the autoencoder\n",
    "    Autoencoder = Autoencoder.fit(X_train, X_train, batch_size = 500, epochs = 3, shuffle = True, validation_data = (X_val, X_val), verbose = 0)\n",
    "    Results = Autoencoder.history\n",
    "    Tuner[learningRate] = Alpha * (Results[validationAccuracy][0] - Results[validationLoss][0]) + Beta * (Results[Accuracy][0] - Results[Loss][0])\n",
    "\n",
    "learningRate = max(Tuner, key = Tuner.get)\n",
    "print(f\"\\n\\nOptimal optimizer, primary weights initialization method, activation functions for hidden layers and learning rate for autoencoder neural network are {optimizerName}, {Initializer}, {Activation} and {learningRate}, respectively.\\n\\nTraining optimal autoencoder...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Autoencoder\n",
    "Autoencoder is now constructed with chosen values of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build main autoencoder\n",
    "Autoencoder = Model(Image, Decoder(Encoder(Image)))\n",
    "\n",
    "# Compile the autoencoder\n",
    "Autoencoder.compile(loss = lossFunction, metrics = [\"accuracy\"], optimizer = Optimizer(learning_rate = learningRate))\n",
    "print(Autoencoder.summary())\n",
    "\n",
    "# Saving model\n",
    "Autoencoder.save(\"Autoencoder.keras\")\n",
    "\n",
    "# Fit the data on the autoencoder\n",
    "# Test set is now the validation set of model!\n",
    "autoencoderHistory = Autoencoder.fit(X_train, X_train, batch_size = 500, epochs = 15, shuffle = True, validation_data = (X_test, X_test), verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Evaluation Plot\n",
    "Final loss value and accuracy of model's perfomance on train set and test set is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history (losses)\n",
    "figure(figsize = (13, 5))\n",
    "ion()\n",
    "plot(autoencoderHistory.history[Loss])\n",
    "plot(autoencoderHistory.history[validationLoss])\n",
    "title(Title)\n",
    "ylabel(\"Loss\")\n",
    "xlabel(\"Epochs\")\n",
    "legend([\"Train Loss\", \"Test Loss\"], loc = \"upper right\")\n",
    "savefig(Title, dpi = 1200)\n",
    "show()\n",
    "\n",
    "# Plot the training history (accuracies)\n",
    "figure(figsize = (13, 5))\n",
    "ion()\n",
    "plot(autoencoderHistory.history[Accuracy])\n",
    "plot(autoencoderHistory.history[validationAccuracy])\n",
    "title(Title)\n",
    "ylabel(\"Accuracy\")\n",
    "xlabel(\"Epochs\")\n",
    "legend([\"Train Accuracy\", \"Test Accuracy\"], loc = \"upper right\")\n",
    "savefig(Title, dpi = 1200)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Encoding\n",
    "Datasets get encoded in their corresponding batch sizes (small enough to get better accuracies and also save time!) which are 60000, 1000 and 1000 for train set, test set and validation set, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding images\n",
    "print(\"\\nEncoding images...\")\n",
    "X_train, X_test, X_val, lossFunction, Title, Tuner = Encoder.predict(X_train, batch_size = 60000, verbose = 0), Encoder.predict(X_test, batch_size = 1000, verbose = 0), Encoder.predict(X_val, batch_size = 1000, verbose = 0), \"sparse_categorical_crossentropy\", \"Classifier Evaluation\", {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Construction & Training\n",
    "A classifier with two hidden layers contain 'ReLU' activation function, and a output layer with 'softmax' activation function; because this function assign decimal probabilities to each class in this multi-class problem. These decimal probabilities must add up to 1. This additional constraint helps training converge more quickly than it otherwise would. \n",
    "Loss function was Sparse Categorical Crossentropy during training this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTraining the classifier...\\n\")\n",
    "# Defining the model\n",
    "Classifier = Sequential()\n",
    "Classifier.add(Dense(512, input_dim = 30, kernel_initializer = \"uniform\", activation = \"relu\"))\n",
    "Classifier.add(Dense(256, kernel_initializer = \"uniform\", activation = \"relu\"))\n",
    "Classifier.add(Dense(10, kernel_initializer = \"uniform\", activation  = \"softmax\"))\n",
    "\n",
    "# Compiling the model\n",
    "Classifier.compile(loss = lossFunction, metrics = [\"accuracy\"], optimizer = \"adam\")#Optimizer(learning_rate = learningRate))\n",
    "\n",
    "print(Classifier.summary())\n",
    "\n",
    "# Training the model\n",
    "classifierHistory = Classifier.fit(X_train, y_train, batch_size = 60000, epochs = 30, verbose = 0, validation_data = (X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network based Classifier Evaluation Plot\n",
    "Final loss value and accuracy of model's perfomance on train set and test set is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training history\n",
    "figure(figsize = (13, 5))\n",
    "ion()\n",
    "plot(classifierHistory.history[Loss])\n",
    "plot(classifierHistory.history[validationLoss])\n",
    "title(Title)\n",
    "ylabel(\"Loss\")\n",
    "xlabel(\"Epochs\")\n",
    "legend([\"Train Loss\", \"Test Loss\"], loc = \"upper right\")\n",
    "savefig(Title, dpi = 1200)\n",
    "show()\n",
    "\n",
    "# Plot the training history (accuracies)\n",
    "figure(figsize = (13, 5))\n",
    "ion()\n",
    "plot(classifierHistory.history[Accuracy])\n",
    "plot(classifierHistory.history[validationAccuracy])\n",
    "title(Title)\n",
    "ylabel(\"Accuracy\")\n",
    "xlabel(\"Epochs\")\n",
    "legend([\"Train Accuracy\", \"Test Accuracy\"], loc = \"upper right\")\n",
    "savefig(Title, dpi = 1200)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "According to plots in 'Evaluations' folder, training the tuned autoencoder in 15 epochs makes good accuracy enough.\n",
    "Batch sizes and epochs for both training the autoencoder, classifier and tuning autoencoder was chosen by hand.\n",
    "Tuning classifier would decrease accuracy so it became commented.\n",
    "After testing whole algorithm for times, accuracy settled in interval (4, 6) (two accuracies captured as samples of results are located in 'Evaluations' folder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , Accuracy\n",
    "Title = \"Confusion Matrix\"\n",
    "print(f\"\\nEvaluating classifier...\\n\\nModel's score: {Classifier.evaluate(X_test, y_test, batch_size = 1000, verbose = 0)}\")\n",
    "ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, argmax(Classifier.predict(X_test, batch_size = 1000, verbose = 0), axis = 1))).plot()\n",
    "title(Title)\n",
    "savefig(Title, dpi = 1200)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "Main challenge was to fine tune autoencoder automatically (especially for different architectures) sync encoded images for getting classified with the neural network base classifier. different clustering algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
